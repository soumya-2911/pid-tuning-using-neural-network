# -*- coding: utf-8 -*-
"""using_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ycqHYv9o19b9-jkQusfc9xFuJ0hRcqQL
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def layers(n_in,n_hidden,n_out):
  Weight=[np.random.uniform(0.5,0.6,[n_hidden,n_in]),np.random.uniform(0.5,0.6,[n_out,n_hidden])]
  bias= np.random.uniform(0.5,0.6,[n_hidden,1])

  return Weight,bias

def feedforward(X,W,b):

  x_h=np.matmul(W[0],X)+b
  O_j=1/(1+np.exp(-x_h))

  O_k=np.matmul(W[1],O_j)
  #O_k=1/(1+np.exp(-x_o))

  return O_j,O_k

def backpropagation(X,O_j,O_k,W,b,alpha,beta,delta_j):
  W_=W
  b_=b
  delta_k=delta_j*np.matmul(np.array([np.sum(W[1],axis=0)]),O_k*(1-O_k))[0][0]
  slope_k= delta_k*O_j  # equivalent to element wise multiply and add about axis 1

  delta_j= np.matmul(delta_k*np.array([np.sum(W[1],axis=0)]),O_j*(1-O_j))[0][0] # make picture
  slope_j= delta_j*(1/(1+np.exp(-X)))

  W_[0] = W_[0] + alpha*slope_j.T
  W_[1] = W_[1] + alpha*slope_k.T
  b_= b_ - beta*delta_j

  #print(delta_j)

  return W_,b_,delta_j





W,b=layers(2,3,3)
#O_j,O_k=feedforward(X,W,b)


#backpropagation(X,O_j,O_k,W,b,1,2)

#print(W[1]*O_k*(1-O_k))

import sympy as sym

# these needs to be predicted

t = sym.Symbol('t', positive = True)
s = sym.Symbol('s', positive = True)

plant_func= 1/s
target=5

r_s= target/s




epoch=10

k_p=0
k_i= 0
k_d=10
y_e_t=[]

delta_j=0.01
W_new,b_new=W,b

for i in range(epoch):



  # plant function part put directly the u_t and c_t value that we got from experiment
  controller = k_p + k_i/s + k_d*s
  G_s= plant_func*controller
  c_s=G_s*r_s/(1 + G_s)
  e_s = r_s - c_s

  c_t = sym.inverse_laplace_transform(c_s, s, t) #process was ran in real life this G_s is unknown to us we have access to c_t we will also have direct access to u_t
  #so in real life we dont have to evaluate at t=1 etc . Just take output and make X
  e_t =sym.inverse_laplace_transform(e_s.simplify(), s, t)
  u_t= sym.inverse_laplace_transform((controller*e_s).simplify(), s, t)

  #neural network part
  X= np.array([[float(u_t.subs(t,1)), float(c_t.subs(t,1))]]).T

  O_j,O_k=feedforward(X,W,b)
  k_p=O_k[0][0]
  k_i=O_k[1][0]
  k_d=O_k[2][0]
  W_new,b_new,delta_j=backpropagation(X,O_j,O_k,W,b,2,3,delta_j)


  y_e_t.append(target-float(c_t.subs(t,1)))
  print(float(c_t.subs(t,1)))

"""**Practical code**"""

import sympy as sym

# these needs to be predicted

target=130



epoch=10

k_p=1.0753419642376332
k_i= 1.0753419642376332
k_d=1.0753419642376332
y_e_t=[]

delta_j=0.01
W_new,b_new=W,b

for i in range(1):






  c_t=2.77
  #neural network part
  X= np.array([[255,c_t]]).T

  O_j,O_k=feedforward(X,W,b)
  k_p=O_k[0][0]
  k_i=O_k[1][0]
  k_d=O_k[2][0]
  W_new,b_new,delta_j=backpropagation(X,O_j,O_k,W,b,2,3,delta_j)


  y_e_t.append(target-c_t)
  print(k_p,k_i,k_d)

"""**Countinuation of modelled code**"""

t = sym.Symbol('t', positive = True)
s = sym.Symbol('s', positive = True)
y_c_t=[0]
time=30
controller = k_p + k_i/s + k_d*s
print(controller)
G_s= plant_func*controller
c_s=G_s*r_s/(1 + G_s)
c_t = sym.inverse_laplace_transform(c_s.simplify(), s, t) # this calculation will be hidden from us in real life we could only see the output

for i in range(1,time):
  y_c_t.append(float(c_t.subs(t,i)))

# error vs epoch graph
x_epoch=np.arange(0,epoch)
plt.plot(x_epoch,y_e_t,color='g')
plt.xlabel("epoch")
plt.ylabel("error (m)")
plt.show()

# output vs t
t=np.arange(0,time)
tgt=np.linspace(target, target, num=time)
plt.plot(t,tgt,color='r')
plt.plot(t,np.array(y_c_t),color='g')
plt.xlabel("time")
plt.ylabel("c_t")
plt.show()

"""**Remember it takes infinite time. So increase k_d to increase damping and lower the k_i value at the end**"""

print(k_p,k_i,k_d)